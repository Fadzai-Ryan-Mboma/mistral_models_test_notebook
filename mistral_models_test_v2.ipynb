{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f380b8c6",
   "metadata": {},
   "source": [
    "# Mistral AI Models Test Notebook (Text Â· Reasoning Â· Multimodal Â· Embeddings)\n",
    "\n",
    "\n",
    "This notebook helps you **try different Mistral AI models by category** (Text, Reasoning, Multimodal, and Embeddings), with **pricing snapshots** and ready-to-run test cells using the latest Python SDK style.\n",
    "\n",
    "> **Sources for pricing & models (check for updates):**\n",
    "> - Mistral AI API Pricing (official): https://mistral.ai/pricing/\n",
    "> - Platform documentation: https://docs.mistral.ai/\n",
    "> - API reference: https://docs.mistral.ai/api/\n",
    "\n",
    "> âš ï¸ **Always verify prices** on the official pricing page before production use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd8238d",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "1. Install the latest Mistral SDK:\n",
    "   ```bash\n",
    "   pip install --upgrade mistralai\n",
    "   ```\n",
    "\n",
    "2. Export your API key (or use `.env`):\n",
    "   ```bash\n",
    "   export MISTRAL_API_KEY=\"your_api_key\"\n",
    "   ```\n",
    "\n",
    "3. Run cells below. Networking must be enabled in your environment to call the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2be8a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: !pip install python-dotenv mistralai pandas matplotlib ipywidgets\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pathlib\n",
    "import math\n",
    "import base64\n",
    "from typing import Dict, Any, Optional, List, Union\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    # Import dotenv for loading .env file if available\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    print(\"dotenv not available. Install with `pip install python-dotenv`\")\n",
    "\n",
    "try:\n",
    "    from mistralai.client import MistralClient\n",
    "    from mistralai.models.chat_completion import ChatMessage\n",
    "    from mistralai.exceptions import MistralAPIException\n",
    "    \n",
    "    # Initialize the Mistral client\n",
    "    api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"âš ï¸ MISTRAL_API_KEY environment variable not found. Please set it before proceeding.\")\n",
    "    else:\n",
    "        client = MistralClient(api_key=api_key)\n",
    "        print(\"âœ… MistralClient initialized successfully.\")\n",
    "except ImportError:\n",
    "    print(\"Mistral SDK not available. Install with `pip install mistralai`\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing MistralClient: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aa3539",
   "metadata": {},
   "source": [
    "## 1) Pricing snapshot (as of this notebook's creation)\n",
    "\n",
    "> Verify current prices at https://mistral.ai/pricing/ before use.\n",
    "\n",
    "We include commonly used models across categories. All Mistral AI models bill **per token** (input and output tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64433eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Updated pricing snapshot â€” verify on https://mistral.ai/pricing/ before production.\n",
    "# Rates are per 1M tokens.\n",
    "\n",
    "PRICING = [\n",
    "    # ---------- TEXT MODELS ----------\n",
    "    # Mistral Large Family\n",
    "    {\"model\": \"mistral-large-latest\", \"category\": \"text\", \"input\": 8.00, \"output\": 24.00, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"Most capable model for complex tasks\"},\n",
    "    {\"model\": \"mistral-large-2407\", \"category\": \"text\", \"input\": 8.00, \"output\": 24.00, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"July 2024 model\"},\n",
    "    \n",
    "    # Mistral Medium Family\n",
    "    {\"model\": \"mistral-medium-latest\", \"category\": \"text\", \"input\": 2.70, \"output\": 8.10, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"Balance between capabilities and cost\"},\n",
    "    {\"model\": \"mistral-medium-2407\", \"category\": \"text\", \"input\": 2.70, \"output\": 8.10, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"July 2024 model\"},\n",
    "    \n",
    "    # Mistral Small Family\n",
    "    {\"model\": \"mistral-small-latest\", \"category\": \"text\", \"input\": 0.70, \"output\": 2.10, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"Cost-effective for general tasks\"},\n",
    "    {\"model\": \"mistral-small-2407\", \"category\": \"text\", \"input\": 0.70, \"output\": 2.10, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"July 2024 model\"},\n",
    "    \n",
    "    # Mistral Tiny Family\n",
    "    {\"model\": \"mistral-tiny-2407\", \"category\": \"text\", \"input\": 0.14, \"output\": 0.42, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"Most efficient model for simple tasks\"},\n",
    "    \n",
    "    # Open Models\n",
    "    {\"model\": \"open-mistral-7b\", \"category\": \"text\", \"input\": 0.25, \"output\": 0.25, \"context_length\": 8192, \"unit\": \"USD / 1M tokens\", \"notes\": \"Open-weight 7B model\"},\n",
    "    {\"model\": \"open-mixtral-8x7b\", \"category\": \"text\", \"input\": 0.70, \"output\": 0.70, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"Open-weight MoE model\"},\n",
    "    \n",
    "    # ---------- MULTIMODAL MODELS ----------\n",
    "    {\"model\": \"mistral-large-vision-2407\", \"category\": \"multimodal\", \"input\": 8.00, \"output\": 24.00, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"Vision model with image understanding\"},\n",
    "    \n",
    "    # ---------- EMBEDDING MODELS ----------\n",
    "    {\"model\": \"mistral-embed\", \"category\": \"embedding\", \"input\": 0.10, \"output\": None, \"context_length\": 8192, \"unit\": \"USD / 1M tokens\", \"notes\": \"Text embedding model (1024D)\"},\n",
    "    \n",
    "    # ---------- REASONING MODELS ----------\n",
    "    # NOTE: Reasoning capability is integrated into the main models, particularly large & medium\n",
    "    {\"model\": \"mistral-large-latest\", \"category\": \"reasoning\", \"input\": 8.00, \"output\": 24.00, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"Most capable for complex reasoning\"},\n",
    "    {\"model\": \"mistral-medium-latest\", \"category\": \"reasoning\", \"input\": 2.70, \"output\": 8.10, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"Good reasoning with balanced cost\"}\n",
    "]\n",
    "\n",
    "df_pricing = pd.DataFrame(PRICING)\n",
    "df_pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496fd13a",
   "metadata": {},
   "source": [
    "## 2) Recommendations & light rankings\n",
    "\n",
    "Below is a simple, **opinionated** ranking by category. Adjust for your workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0281b9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Heuristic ranking (lower score = better considering price/perf for the category)\n",
    "RANKING = {\n",
    "    \"text\": [\n",
    "        {\"model\": \"mistral-large-latest\", \"score\": 1, \"why\": \"Best overall performance for challenging tasks\"},\n",
    "        {\"model\": \"mistral-medium-latest\", \"score\": 2, \"why\": \"Great balance of capability and cost efficiency\"},\n",
    "        {\"model\": \"mistral-small-latest\", \"score\": 3, \"why\": \"Good performance for most general tasks at lower cost\"}\n",
    "    ],\n",
    "    \"multimodal\": [\n",
    "        {\"model\": \"mistral-large-vision-2407\", \"score\": 1, \"why\": \"High-performance vision capabilities\"}\n",
    "    ],\n",
    "    \"reasoning\": [\n",
    "        {\"model\": \"mistral-large-latest\", \"score\": 1, \"why\": \"Best for complex reasoning and multi-step problem solving\"},\n",
    "        {\"model\": \"mistral-medium-latest\", \"score\": 2, \"why\": \"Good reasoning capabilities with better cost efficiency\"},\n",
    "        {\"model\": \"mistral-small-latest\", \"score\": 3, \"why\": \"Adequate for simpler reasoning tasks at an affordable price\"}\n",
    "    ],\n",
    "    \"embedding\": [\n",
    "        {\"model\": \"mistral-embed\", \"score\": 1, \"why\": \"High-quality embeddings for semantic search and RAG\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "def ranking_table(category: str):\n",
    "    rows = RANKING.get(category, [])\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "display(ranking_table(\"text\"))\n",
    "display(ranking_table(\"multimodal\"))\n",
    "display(ranking_table(\"reasoning\"))\n",
    "display(ranking_table(\"embedding\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee4fef4",
   "metadata": {},
   "source": [
    "## 3) Test harness\n",
    "\n",
    "Utilities to run quick tests against selected models using the **Chat API**.\n",
    "\n",
    "> Make sure `MISTRAL_API_KEY` is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff28c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "import time\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "def run_chat_with_model(model: str, messages: List[ChatMessage], temperature: float = 0.7, max_tokens: Optional[int] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Sends a list of chat messages and returns the model's response and usage.\"\"\"\n",
    "    assert client is not None, \"MistralClient not initialized. Set MISTRAL_API_KEY and initialize client.\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        usage = response.usage\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"content\": content,\n",
    "            \"usage\": usage,\n",
    "            \"model\": model,\n",
    "            \"finish_reason\": response.choices[0].finish_reason\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "def run_text_prompt(model: str, prompt: str, system_prompt: str = None, temperature: float = 0.7, max_tokens: Optional[int] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Sends a simple text prompt and returns the text output and usage.\"\"\"\n",
    "    messages = []\n",
    "    \n",
    "    if system_prompt:\n",
    "        messages.append(ChatMessage(role=\"system\", content=system_prompt))\n",
    "        \n",
    "    messages.append(ChatMessage(role=\"user\", content=prompt))\n",
    "    \n",
    "    return run_chat_with_model(model, messages, temperature, max_tokens)\n",
    "\n",
    "def estimate_cost(model: str, usage: Dict[str, Any], pricing_df) -> Tuple[float, float, float]:\n",
    "    \"\"\"Estimates input, output, and total costs given usage metrics and our pricing table.\"\"\"\n",
    "    if not usage:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    \n",
    "    prompt_tokens = usage.get(\"prompt_tokens\", 0)\n",
    "    completion_tokens = usage.get(\"completion_tokens\", 0)\n",
    "\n",
    "    # Find the model in pricing dataframe\n",
    "    model_row = pricing_df[pricing_df[\"model\"] == model]\n",
    "    if model_row.empty:\n",
    "        # Try to find by prefix match\n",
    "        for idx, row in pricing_df.iterrows():\n",
    "            if model.startswith(row[\"model\"].split(\"-latest\")[0]):\n",
    "                model_row = pricing_df.iloc[[idx]]\n",
    "                break\n",
    "    \n",
    "    if model_row.empty:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    input_rate = model_row.iloc[0][\"input\"] or 0.0\n",
    "    output_rate = model_row.iloc[0][\"output\"] or 0.0\n",
    "    \n",
    "    input_cost = (prompt_tokens / 1_000_000.0) * input_rate\n",
    "    output_cost = (completion_tokens / 1_000_000.0) * output_rate\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    return input_cost, output_cost, total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b253759",
   "metadata": {},
   "source": [
    "### 4A) Text models tests\n",
    "\n",
    "Use different models in the Mistral AI family to compare output quality, performance, and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea203dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a basic prompt with different text models\n",
    "prompt = \"Write a 4-sentence summary of the benefits of Infrastructure as Code (IaC) in Kubernetes environments.\"\n",
    "\n",
    "text_models = [\"mistral-tiny-2407\", \"mistral-small-latest\", \"mistral-medium-latest\", \"mistral-large-latest\"]\n",
    "results = {}\n",
    "\n",
    "for model in text_models:\n",
    "    try:\n",
    "        print(f\"\\nğŸ“ Testing model: {model}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        result = run_text_prompt(model=model, prompt=prompt)\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            print(result[\"content\"])\n",
    "            \n",
    "            # Calculate cost\n",
    "            input_cost, output_cost, total_cost = estimate_cost(model, result[\"usage\"], df_pricing)\n",
    "            \n",
    "            print(f\"\\nğŸ“Š Usage:\")\n",
    "            print(f\"  Prompt tokens: {result['usage'].prompt_tokens}\")\n",
    "            print(f\"  Completion tokens: {result['usage'].completion_tokens}\")\n",
    "            print(f\"  Total tokens: {result['usage'].prompt_tokens + result['usage'].completion_tokens}\")\n",
    "            print(f\"  Estimated cost: ${total_cost:.6f} (${input_cost:.6f} input + ${output_cost:.6f} output)\")\n",
    "            \n",
    "            results[model] = {\n",
    "                \"content\": result[\"content\"],\n",
    "                \"usage\": result[\"usage\"],\n",
    "                \"cost\": total_cost\n",
    "            }\n",
    "        else:\n",
    "            print(f\"âŒ Error: {result['error']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error with model {model}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d53311b",
   "metadata": {},
   "source": [
    "### 4B) Complex Reasoning Tests\n",
    "\n",
    "Test the reasoning capabilities of Mistral AI models with more complex tasks that require multi-step thinking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbafe87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reasoning capabilities with a math problem\n",
    "reasoning_prompt = \"Solve this: If f(x)=2x^2-3x+5, compute f(7). Show steps briefly, then give the final answer on a new line prefixed with 'Answer:'.\"\n",
    "\n",
    "# We'll test with medium and large models which excel at reasoning tasks\n",
    "reasoning_models = [\"mistral-small-latest\", \"mistral-medium-latest\", \"mistral-large-latest\"]\n",
    "\n",
    "for model in reasoning_models:\n",
    "    try:\n",
    "        print(f\"\\nğŸ§  Testing reasoning with: {model}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        result = run_text_prompt(\n",
    "            model=model,\n",
    "            prompt=reasoning_prompt,\n",
    "            temperature=0.3,  # Lower temperature for more precise reasoning\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            print(result[\"content\"])\n",
    "            \n",
    "            # Calculate cost\n",
    "            input_cost, output_cost, total_cost = estimate_cost(model, result[\"usage\"], df_pricing)\n",
    "            \n",
    "            print(f\"\\nğŸ“Š Usage:\")\n",
    "            print(f\"  Prompt tokens: {result['usage'].prompt_tokens}\")\n",
    "            print(f\"  Completion tokens: {result['usage'].completion_tokens}\")\n",
    "            print(f\"  Estimated cost: ${total_cost:.6f}\")\n",
    "        else:\n",
    "            print(f\"âŒ Error: {result['error']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error with model {model}: {e}\")\n",
    "        \n",
    "# Now let's test a more complex reasoning problem - a logical puzzle\n",
    "complex_reasoning_prompt = \"\"\"\n",
    "Solve this logical puzzle step by step:\n",
    "\n",
    "Five friends (Alex, Blake, Casey, Dana, and Elliot) are sitting in a row at a movie theater. \n",
    "We know the following:\n",
    "- Alex is sitting to the left of Blake\n",
    "- Casey is sitting next to Dana\n",
    "- Elliot is sitting at one of the ends\n",
    "- Blake is sitting next to Elliot\n",
    "- Alex and Dana are not sitting next to each other\n",
    "\n",
    "What is the seating arrangement from left to right?\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nğŸ§© Complex Logical Reasoning Test\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Testing with the most capable model for complex reasoning\n",
    "try:\n",
    "    result = run_text_prompt(\n",
    "        model=\"mistral-large-latest\",\n",
    "        prompt=complex_reasoning_prompt,\n",
    "        temperature=0.3,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        print(result[\"content\"])\n",
    "        \n",
    "        # Calculate cost\n",
    "        input_cost, output_cost, total_cost = estimate_cost(\"mistral-large-latest\", result[\"usage\"], df_pricing)\n",
    "        print(f\"\\nğŸ“Š Usage:\")\n",
    "        print(f\"  Prompt tokens: {result['usage'].prompt_tokens}\")\n",
    "        print(f\"  Completion tokens: {result['usage'].completion_tokens}\")\n",
    "        print(f\"  Estimated cost: ${total_cost:.6f}\")\n",
    "    else:\n",
    "        print(f\"âŒ Error: {result['error']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error with complex reasoning: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d1dd86",
   "metadata": {},
   "source": [
    "### 4C) Multimodal (Vision) Tests\n",
    "\n",
    "Test the vision capabilities of Mistral AI models with image understanding tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d6ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_to_base64(image_path):\n",
    "    \"\"\"Encodes an image file to base64 format.\"\"\"\n",
    "    import base64\n",
    "    import os\n",
    "    \n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
    "    \n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def run_vision_query(image_path, prompt, model=\"mistral-large-vision-2407\", temperature=0.5):\n",
    "    \"\"\"Send a vision query with an image and a prompt.\"\"\"\n",
    "    try:\n",
    "        # Encode image\n",
    "        base64_image = encode_image_to_base64(image_path)\n",
    "        \n",
    "        # Create a multimodal message\n",
    "        messages = [\n",
    "            ChatMessage(\n",
    "                role=\"user\",\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image\", \"image\": {\"data\": base64_image, \"format\": \"jpeg\"}}\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Call the API\n",
    "        response = client.chat(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"content\": response.choices[0].message.content,\n",
    "            \"usage\": response.usage\n",
    "        }\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "# Test vision capabilities if an image is available\n",
    "# First check if a sample image exists\n",
    "import os\n",
    "\n",
    "sample_image_path = \"sample_image.jpg\"  # Replace with your image path\n",
    "\n",
    "if os.path.exists(sample_image_path):\n",
    "    print(\"ğŸ–¼ï¸ Testing vision capabilities with sample image\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    vision_prompts = [\n",
    "        \"Describe what you see in this image in detail.\",\n",
    "        \"What objects can you identify in this image?\",\n",
    "        \"What is the main subject of this image? Analyze its composition.\"\n",
    "    ]\n",
    "    \n",
    "    for prompt in vision_prompts:\n",
    "        try:\n",
    "            print(f\"\\nğŸ“ Vision prompt: {prompt}\")\n",
    "            result = run_vision_query(sample_image_path, prompt)\n",
    "            \n",
    "            if result[\"success\"]:\n",
    "                print(f\"\\nğŸ’¬ Response:\")\n",
    "                print(result[\"content\"])\n",
    "                \n",
    "                # Calculate cost\n",
    "                input_cost, output_cost, total_cost = estimate_cost(\"mistral-large-vision-2407\", result[\"usage\"], df_pricing)\n",
    "                print(f\"\\nğŸ“Š Usage:\")\n",
    "                print(f\"  Prompt tokens: {result['usage'].prompt_tokens}\")\n",
    "                print(f\"  Completion tokens: {result['usage'].completion_tokens}\")\n",
    "                print(f\"  Estimated cost: ${total_cost:.6f}\")\n",
    "            else:\n",
    "                print(f\"âŒ Error: {result['error']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error with vision query: {e}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Sample image not found at {sample_image_path}\")\n",
    "    print(\"To test vision capabilities, please provide a sample image.\")\n",
    "    print(\"You can download a sample image and place it in the same directory as this notebook.\")\n",
    "    print(\"Then run this cell again to test the vision capabilities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416cd7fb",
   "metadata": {},
   "source": [
    "### 4D) Embeddings Tests\n",
    "\n",
    "Test the embedding capabilities of Mistral AI for semantic search and similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbc107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_embedding(text, model=\"mistral-embed\"):\n",
    "    \"\"\"Get embeddings for a single text using Mistral's embedding model.\"\"\"\n",
    "    try:\n",
    "        response = client.embeddings(model=model, input=[text])\n",
    "        embedding = response.data[0].embedding\n",
    "        usage = response.usage\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"embedding\": embedding,\n",
    "            \"usage\": usage\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "def get_embeddings_batch(texts, model=\"mistral-embed\"):\n",
    "    \"\"\"Get embeddings for multiple texts in a batch.\"\"\"\n",
    "    try:\n",
    "        response = client.embeddings(model=model, input=texts)\n",
    "        embeddings = [data.embedding for data in response.data]\n",
    "        usage = response.usage\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"embeddings\": embeddings,\n",
    "            \"usage\": usage\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Test embedding capabilities\n",
    "print(\"ğŸ” Testing embedding capabilities\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sample texts for embedding comparison\n",
    "test_texts = [\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"AI and machine learning are transforming technology.\",\n",
    "    \"The weather today is sunny and warm.\",\n",
    "    \"Python is a popular programming language for data science.\",\n",
    "    \"Data science involves analyzing and interpreting complex data.\"\n",
    "]\n",
    "\n",
    "# Get embeddings for all texts\n",
    "try:\n",
    "    result = get_embeddings_batch(test_texts)\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        embeddings = result[\"embeddings\"]\n",
    "        usage = result[\"usage\"]\n",
    "        \n",
    "        print(f\"âœ… Generated embeddings for {len(embeddings)} texts\")\n",
    "        print(f\"   Embedding dimension: {len(embeddings[0])}\")\n",
    "        print(f\"   Total tokens used: {usage.total_tokens}\")\n",
    "        \n",
    "        # Calculate cost\n",
    "        input_cost, output_cost, total_cost = estimate_cost(\"mistral-embed\", usage, df_pricing)\n",
    "        print(f\"   Estimated cost: ${total_cost:.6f}\")\n",
    "        \n",
    "        # Calculate and display similarities\n",
    "        print(\"\\nğŸ“Š Similarity Matrix:\")\n",
    "        print(\"\" + \"-\" * 80)\n",
    "        \n",
    "        similarity_matrix = []\n",
    "        for i, text_a in enumerate(test_texts):\n",
    "            row = []\n",
    "            for j, text_b in enumerate(test_texts):\n",
    "                similarity = cosine_similarity(embeddings[i], embeddings[j])\n",
    "                row.append(similarity)\n",
    "            similarity_matrix.append(row)\n",
    "        \n",
    "        # Create a DataFrame for better visualization\n",
    "        df_similarity = pd.DataFrame(\n",
    "            similarity_matrix,\n",
    "            index=[f\"Text {i+1}\" for i in range(len(test_texts))],\n",
    "            columns=[f\"Text {i+1}\" for i in range(len(test_texts))]\n",
    "        )\n",
    "        \n",
    "        print(df_similarity.round(3))\n",
    "        \n",
    "        # Find most similar pairs (excluding self-similarity)\n",
    "        print(\"\\nğŸ”— Most similar text pairs:\")\n",
    "        max_similarity = 0\n",
    "        most_similar_pair = None\n",
    "        \n",
    "        for i in range(len(test_texts)):\n",
    "            for j in range(i+1, len(test_texts)):\n",
    "                similarity = similarity_matrix[i][j]\n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    most_similar_pair = (i, j)\n",
    "        \n",
    "        if most_similar_pair:\n",
    "            i, j = most_similar_pair\n",
    "            print(f\"   Text {i+1} & Text {j+1}: {max_similarity:.3f}\")\n",
    "            print(f\"   '{test_texts[i]}'\")\n",
    "            print(f\"   '{test_texts[j]}'\")\n",
    "        \n",
    "        print(\"\\nğŸ“ Text Reference:\")\n",
    "        for i, text in enumerate(test_texts):\n",
    "            print(f\"   Text {i+1}: {text}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"âŒ Error: {result['error']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error with embeddings: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8462b8",
   "metadata": {},
   "source": [
    "## 5) Summary & Cost Analysis\n",
    "\n",
    "Compare results across all tested models and analyze costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f22eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell would summarize all the results from the previous tests\n",
    "# You can expand this section to create comprehensive comparisons\n",
    "\n",
    "print(\"ğŸ“‹ Test Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'results' in locals() and results:\n",
    "    print(\"\\nğŸ’° Cost Comparison for Text Models:\")\n",
    "    for model, data in results.items():\n",
    "        print(f\"  {model}: ${data['cost']:.6f}\")\n",
    "    \n",
    "    # Find the most cost-effective model\n",
    "    cheapest_model = min(results.items(), key=lambda x: x[1]['cost'])\n",
    "    most_expensive_model = max(results.items(), key=lambda x: x[1]['cost'])\n",
    "    \n",
    "    print(f\"\\nğŸ† Most cost-effective: {cheapest_model[0]} (${cheapest_model[1]['cost']:.6f})\")\n",
    "    print(f\"ğŸ’ Most expensive: {most_expensive_model[0]} (${most_expensive_model[1]['cost']:.6f})\")\n",
    "    \n",
    "    cost_ratio = most_expensive_model[1]['cost'] / cheapest_model[1]['cost']\n",
    "    print(f\"ğŸ“Š Cost ratio (most expensive vs cheapest): {cost_ratio:.1f}x\")\n",
    "else:\n",
    "    print(\"âš ï¸ No test results available. Run the text model tests above first.\")\n",
    "\n",
    "print(\"\\nâœ… Testing complete! Review the results above to choose the best model for your use case.\")\n",
    "print(\"\\nğŸ’¡ Remember to:\")\n",
    "print(\"   - Verify current pricing on https://mistral.ai/pricing/\")\n",
    "print(\"   - Test with your specific use cases\")\n",
    "print(\"   - Consider both cost and quality for your requirements\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
