{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52ddc4a6",
   "metadata": {},
   "source": [
    "# Mistral AI Models Test Notebook (Text ¬∑ Reasoning ¬∑ Multimodal ¬∑ Embeddings)\n",
    "\n",
    "\n",
    "This notebook helps you **try different Mistral AI models by category** (Text, Reasoning, Multimodal, and Embeddings), with **pricing snapshots** and ready-to-run test cells using the latest Python SDK style.\n",
    "\n",
    "> **Sources for pricing & models (check for updates):**\n",
    "> - Mistral AI API Pricing (official): https://mistral.ai/pricing/\n",
    "> - Platform documentation: https://docs.mistral.ai/\n",
    "> - API reference: https://docs.mistral.ai/api/\n",
    "\n",
    "> ‚ö†Ô∏è **Always verify prices** on the official pricing page before production use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb9195e",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "1. Install the latest Mistral SDK:\n",
    "   ```bash\n",
    "   pip install --upgrade mistralai\n",
    "   ```\n",
    "\n",
    "2. Export your API key (or use `.env`):\n",
    "   ```bash\n",
    "   export MISTRAL_API_KEY=\"your_api_key\"\n",
    "   ```\n",
    "\n",
    "3. Run cells below. Networking must be enabled in your environment to call the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969af958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: !pip install python-dotenv mistralai pandas matplotlib ipywidgets\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pathlib\n",
    "import math\n",
    "import base64\n",
    "from typing import Dict, Any, Optional, List, Union\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    # Import dotenv for loading .env file if available\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    print(\"dotenv not available. Install with `pip install python-dotenv`\")\n",
    "\n",
    "try:\n",
    "    from mistralai.client import MistralClient\n",
    "    from mistralai.models.chat_completion import ChatMessage\n",
    "    from mistralai.exceptions import MistralAPIException\n",
    "    \n",
    "    # Initialize the Mistral client\n",
    "    api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"‚ö†Ô∏è MISTRAL_API_KEY environment variable not found. Please set it before proceeding.\")\n",
    "    else:\n",
    "        client = MistralClient(api_key=api_key)\n",
    "        print(\"‚úÖ MistralClient initialized successfully.\")\n",
    "except ImportError:\n",
    "    print(\"Mistral SDK not available. Install with `pip install mistralai`\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing MistralClient: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea57a9a",
   "metadata": {},
   "source": [
    "## 1) Pricing snapshot (as of this notebook's creation)\n",
    "\n",
    "> Verify current prices at https://mistral.ai/pricing/ before use.\n",
    "\n",
    "We include commonly used models across categories. All Mistral AI models bill **per token** (input and output tokens).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59bcdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Updated pricing snapshot ‚Äî verify on https://mistral.ai/pricing/ before production.\n",
    "# Rates are per 1M tokens.\n",
    "\n",
    "PRICING = [\n",
    "    # ---------- TEXT MODELS ----------\n",
    "    # Mistral Large Family\n",
    "    {\"model\": \"mistral-large-latest\", \"category\": \"text\", \"input\": 8.00, \"output\": 24.00, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"Most capable model for complex tasks\"},\n",
    "    {\"model\": \"mistral-large-2407\", \"category\": \"text\", \"input\": 8.00, \"output\": 24.00, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"July 2024 model\"},\n",
    "    \n",
    "    # Mistral Medium Family\n",
    "    {\"model\": \"mistral-medium-latest\", \"category\": \"text\", \"input\": 2.70, \"output\": 8.10, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"Balance between capabilities and cost\"},\n",
    "    {\"model\": \"mistral-medium-2407\", \"category\": \"text\", \"input\": 2.70, \"output\": 8.10, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"July 2024 model\"},\n",
    "    \n",
    "    # Mistral Small Family\n",
    "    {\"model\": \"mistral-small-latest\", \"category\": \"text\", \"input\": 0.70, \"output\": 2.10, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"Cost-effective for general tasks\"},\n",
    "    {\"model\": \"mistral-small-2407\", \"category\": \"text\", \"input\": 0.70, \"output\": 2.10, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"July 2024 model\"},\n",
    "    \n",
    "    # Mistral Tiny Family\n",
    "    {\"model\": \"mistral-tiny-2407\", \"category\": \"text\", \"input\": 0.14, \"output\": 0.42, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"Most efficient model for simple tasks\"},\n",
    "    \n",
    "    # Open Models\n",
    "    {\"model\": \"open-mistral-7b\", \"category\": \"text\", \"input\": 0.25, \"output\": 0.25, \"context_length\": 8192, \"unit\": \"USD / 1M tokens\", \"notes\": \"Open-weight 7B model\"},\n",
    "    {\"model\": \"open-mixtral-8x7b\", \"category\": \"text\", \"input\": 0.70, \"output\": 0.70, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"Open-weight MoE model\"},\n",
    "    \n",
    "    # ---------- MULTIMODAL MODELS ----------\n",
    "    {\"model\": \"mistral-large-vision-2407\", \"category\": \"multimodal\", \"input\": 8.00, \"output\": 24.00, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"Vision model with image understanding\"},\n",
    "    \n",
    "    # ---------- EMBEDDING MODELS ----------\n",
    "    {\"model\": \"mistral-embed\", \"category\": \"embedding\", \"input\": 0.10, \"output\": None, \"context_length\": 8192, \"unit\": \"USD / 1M tokens\", \"notes\": \"Text embedding model (1024D)\"},\n",
    "    \n",
    "    # ---------- REASONING MODELS ----------\n",
    "    # NOTE: Reasoning capability is integrated into the main models, particularly large & medium\n",
    "    {\"model\": \"mistral-large-latest\", \"category\": \"reasoning\", \"input\": 8.00, \"output\": 24.00, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"Most capable for complex reasoning\"},\n",
    "    {\"model\": \"mistral-medium-latest\", \"category\": \"reasoning\", \"input\": 2.70, \"output\": 8.10, \"context_length\": 32768, \"unit\": \"USD / 1M tokens\", \"notes\": \"Good reasoning with balanced cost\"}\n",
    "]\n",
    "\n",
    "df_pricing = pd.DataFrame(PRICING)\n",
    "df_pricing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43832e86",
   "metadata": {},
   "source": [
    "## 2) Recommendations & light rankings\n",
    "\n",
    "Below is a simple, **opinionated** ranking by category. Adjust for your workload.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7bdd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Heuristic ranking (lower score = better considering price/perf for the category)\n",
    "RANKING = {\n",
    "    \"text\": [\n",
    "        {\"model\": \"mistral-large-latest\", \"score\": 1, \"why\": \"Best overall performance for challenging tasks\"},\n",
    "        {\"model\": \"mistral-medium-latest\", \"score\": 2, \"why\": \"Great balance of capability and cost efficiency\"},\n",
    "        {\"model\": \"mistral-small-latest\", \"score\": 3, \"why\": \"Good performance for most general tasks at lower cost\"}\n",
    "    ],\n",
    "    \"multimodal\": [\n",
    "        {\"model\": \"mistral-large-vision-2407\", \"score\": 1, \"why\": \"High-performance vision capabilities\"}\n",
    "    ],\n",
    "    \"reasoning\": [\n",
    "        {\"model\": \"mistral-large-latest\", \"score\": 1, \"why\": \"Best for complex reasoning and multi-step problem solving\"},\n",
    "        {\"model\": \"mistral-medium-latest\", \"score\": 2, \"why\": \"Good reasoning capabilities with better cost efficiency\"},\n",
    "        {\"model\": \"mistral-small-latest\", \"score\": 3, \"why\": \"Adequate for simpler reasoning tasks at an affordable price\"}\n",
    "    ],\n",
    "    \"embedding\": [\n",
    "        {\"model\": \"mistral-embed\", \"score\": 1, \"why\": \"High-quality embeddings for semantic search and RAG\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "def ranking_table(category: str):\n",
    "    rows = RANKING.get(category, [])\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "display(ranking_table(\"text\"))\n",
    "display(ranking_table(\"multimodal\"))\n",
    "display(ranking_table(\"reasoning\"))\n",
    "display(ranking_table(\"embedding\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a0f02d",
   "metadata": {},
   "source": [
    "## 3) Test harness\n",
    "\n",
    "Utilities to run quick tests against selected models using the **Chat API**.\n",
    "\n",
    "> Make sure `MISTRAL_API_KEY` is set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "import time\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "def run_chat_with_model(model: str, messages: List[ChatMessage], temperature: float = 0.7, max_tokens: Optional[int] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Sends a list of chat messages and returns the model's response and usage.\"\"\"\n",
    "    assert client is not None, \"MistralClient not initialized. Set MISTRAL_API_KEY and initialize client.\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        usage = response.usage\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"content\": content,\n",
    "            \"usage\": usage,\n",
    "            \"model\": model,\n",
    "            \"finish_reason\": response.choices[0].finish_reason\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "def run_text_prompt(model: str, prompt: str, system_prompt: str = None, temperature: float = 0.7, max_tokens: Optional[int] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Sends a simple text prompt and returns the text output and usage.\"\"\"\n",
    "    messages = []\n",
    "    \n",
    "    if system_prompt:\n",
    "        messages.append(ChatMessage(role=\"system\", content=system_prompt))\n",
    "        \n",
    "    messages.append(ChatMessage(role=\"user\", content=prompt))\n",
    "    \n",
    "    return run_chat_with_model(model, messages, temperature, max_tokens)\n",
    "\n",
    "def estimate_cost(model: str, usage: Dict[str, Any], pricing_df) -> Tuple[float, float, float]:\n",
    "    \"\"\"Estimates input, output, and total costs given usage metrics and our pricing table.\"\"\"\n",
    "    if not usage:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    \n",
    "    prompt_tokens = usage.get(\"prompt_tokens\", 0)\n",
    "    completion_tokens = usage.get(\"completion_tokens\", 0)\n",
    "\n",
    "    # Find the model in pricing dataframe\n",
    "    model_row = pricing_df[pricing_df[\"model\"] == model]\n",
    "    if model_row.empty:\n",
    "        # Try to find by prefix match\n",
    "        for idx, row in pricing_df.iterrows():\n",
    "            if model.startswith(row[\"model\"].split(\"-latest\")[0]):\n",
    "                model_row = pricing_df.iloc[[idx]]\n",
    "                break\n",
    "    \n",
    "    if model_row.empty:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    input_rate = model_row.iloc[0][\"input\"] or 0.0\n",
    "    output_rate = model_row.iloc[0][\"output\"] or 0.0\n",
    "    \n",
    "    input_cost = (prompt_tokens / 1_000_000.0) * input_rate\n",
    "    output_cost = (completion_tokens / 1_000_000.0) * output_rate\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    return input_cost, output_cost, total_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edac17e1",
   "metadata": {},
   "source": [
    "### 4A) Text models tests\n",
    "\n",
    "Use different models in the Mistral AI family to compare output quality, performance, and cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3faba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a basic prompt with different text models\n",
    "prompt = \"Write a 4-sentence summary of the benefits of Infrastructure as Code (IaC) in Kubernetes environments.\"\n",
    "\n",
    "text_models = [\"mistral-tiny-2407\", \"mistral-small-latest\", \"mistral-medium-latest\", \"mistral-large-latest\"]\n",
    "results = {}\n",
    "\n",
    "for model in text_models:\n",
    "    try:\n",
    "        print(f\"\\nüìù Testing model: {model}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        result = run_text_prompt(model=model, prompt=prompt)\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            print(result[\"content\"])\n",
    "            \n",
    "            # Calculate cost\n",
    "            input_cost, output_cost, total_cost = estimate_cost(model, result[\"usage\"], df_pricing)\n",
    "            \n",
    "            print(f\"\\nüìä Usage:\")\n",
    "            print(f\"  Prompt tokens: {result['usage'].prompt_tokens}\")\n",
    "            print(f\"  Completion tokens: {result['usage'].completion_tokens}\")\n",
    "            print(f\"  Total tokens: {result['usage'].prompt_tokens + result['usage'].completion_tokens}\")\n",
    "            print(f\"  Estimated cost: ${total_cost:.6f} (${input_cost:.6f} input + ${output_cost:.6f} output)\")\n",
    "            \n",
    "            results[model] = {\n",
    "                \"content\": result[\"content\"],\n",
    "                \"usage\": result[\"usage\"],\n",
    "                \"cost\": total_cost\n",
    "            }\n",
    "        else:\n",
    "            print(f\"‚ùå Error: {result['error']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with model {model}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef09d8e7",
   "metadata": {},
   "source": [
    "### 4B) Complex Reasoning Tests\n",
    "\n",
    "Test the reasoning capabilities of Mistral AI models with more complex tasks that require multi-step thinking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbb23d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reasoning capabilities with a math problem\n",
    "reasoning_prompt = \"Solve this: If f(x)=2x^2-3x+5, compute f(7). Show steps briefly, then give the final answer on a new line prefixed with 'Answer:'.\"\n",
    "\n",
    "# We'll test with medium and large models which excel at reasoning tasks\n",
    "reasoning_models = [\"mistral-small-latest\", \"mistral-medium-latest\", \"mistral-large-latest\"]\n",
    "\n",
    "for model in reasoning_models:\n",
    "    try:\n",
    "        print(f\"\\nüß† Testing reasoning with: {model}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        result = run_text_prompt(\n",
    "            model=model,\n",
    "            prompt=reasoning_prompt,\n",
    "            temperature=0.3,  # Lower temperature for more precise reasoning\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            print(result[\"content\"])\n",
    "            \n",
    "            # Calculate cost\n",
    "            input_cost, output_cost, total_cost = estimate_cost(model, result[\"usage\"], df_pricing)\n",
    "            \n",
    "            print(f\"\\nüìä Usage:\")\n",
    "            print(f\"  Prompt tokens: {result['usage'].prompt_tokens}\")\n",
    "            print(f\"  Completion tokens: {result['usage'].completion_tokens}\")\n",
    "            print(f\"  Estimated cost: ${total_cost:.6f}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Error: {result['error']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with model {model}: {e}\")\n",
    "        \n",
    "# Now let's test a more complex reasoning problem - a logical puzzle\n",
    "complex_reasoning_prompt = \"\"\"\n",
    "Solve this logical puzzle step by step:\n",
    "\n",
    "Five friends (Alex, Blake, Casey, Dana, and Elliot) are sitting in a row at a movie theater. \n",
    "We know the following:\n",
    "- Alex is sitting to the left of Blake\n",
    "- Casey is sitting next to Dana\n",
    "- Elliot is sitting at one of the ends\n",
    "- Blake is sitting next to Elliot\n",
    "- Alex and Dana are not sitting next to each other\n",
    "\n",
    "What is the seating arrangement from left to right?\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüß© Complex Logical Reasoning Test\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Testing with the most capable model for complex reasoning\n",
    "try:\n",
    "    result = run_text_prompt(\n",
    "        model=\"mistral-large-latest\",\n",
    "        prompt=complex_reasoning_prompt,\n",
    "        temperature=0.3,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        print(result[\"content\"])\n",
    "        \n",
    "        # Calculate cost\n",
    "        input_cost, output_cost, total_cost = estimate_cost(\"mistral-large-latest\", result[\"usage\"], df_pricing)\n",
    "        print(f\"\\nüìä Usage:\")\n",
    "        print(f\"  Prompt tokens: {result['usage'].prompt_tokens}\")\n",
    "        print(f\"  Completion tokens: {result['usage'].completion_tokens}\")\n",
    "        print(f\"  Estimated cost: ${total_cost:.6f}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {result['error']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error with complex reasoning: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b829c3",
   "metadata": {},
   "source": [
    "### 4C) Multimodal (Vision) Tests\n",
    "\n",
    "Test the vision capabilities of Mistral AI models with image understanding tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6e02e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_to_base64(image_path):\n",
    "    \"\"\"Encodes an image file to base64 format.\"\"\"\n",
    "    import base64\n",
    "    import os\n",
    "    \n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
    "    \n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def run_vision_query(image_path, prompt, model=\"mistral-large-vision-2407\", temperature=0.5):\n",
    "    \"\"\"Send a vision query with an image and a prompt.\"\"\"\n",
    "    try:\n",
    "        # Encode image\n",
    "        base64_image = encode_image_to_base64(image_path)\n",
    "        \n",
    "        # Create a multimodal message\n",
    "        messages = [\n",
    "            ChatMessage(\n",
    "                role=\"user\",\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image\", \"image\": {\"data\": base64_image, \"format\": \"jpeg\"}}\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Call the API\n",
    "        response = client.chat(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"content\": response.choices[0].message.content,\n",
    "            \"usage\": response.usage\n",
    "        }\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "# Test vision capabilities if an image is available\n",
    "# First check if a sample image exists\n",
    "import os\n",
    "\n",
    "sample_image_path = \"sample_image.jpg\"  # Replace with your image path\n",
    "\n",
    "if os.path.exists(sample_image_path):\n",
    "    print(\"üñºÔ∏è Testing vision capabilities with sample image\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    vision_prompts = [\n",
    "        \"Describe what you see in this image in detail.\",\n",
    "        \"What objects can you identify in this image?\",\n",
    "        \"What is the main subject of this image? Analyze its composition.\"\n",
    "    ]\n",
    "    \n",
    "    for prompt in vision_prompts:\n",
    "        try:\n",
    "            print(f\"\\nüìù Vision prompt: {prompt}\")\n",
    "            result = run_vision_query(sample_image_path, prompt)\n",
    "            \n",
    "            if result[\"success\"]:\n",
    "                print(f\"\\nüí¨ Response:\")\n",
    "                print(result[\"content\"])\n",
    "                \n",
    "                # Calculate cost\n",
    "                input_cost, output_cost, total_cost = estimate_cost(\"mistral-large-vision-2407\", result[\"usage\"], df_pricing)\n",
    "                print(f\"\\nüìä Usage:\")\n",
    "                print(f\"  Prompt tokens: {result['usage'].prompt_tokens}\")\n",
    "                print(f\"  Completion tokens: {result['usage'].completion_tokens}\")\n",
    "                print(f\"  Estimated cost: ${total_cost:.6f}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Error: {result['error']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with vision query: {e}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Sample image not found at {sample_image_path}\")\n",
    "    print(\"To test vision capabilities, please provide a sample image.\")\n",
    "    print(\"You can download a sample image and place it in the same directory as this notebook.\")\n",
    "    print(\"Then run this cell again to test the vision capabilities.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cfe09d",
   "metadata": {},
   "source": [
    "### 4D) Embeddings Tests\n",
    "\n",
    "Test the embedding capabilities of Mistral AI for semantic search and similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_embedding(text, model=\"mistral-embed\"):\n",
    "    \"\"\"Get embeddings for a single text using Mistral's embedding model.\"\"\"\n",
    "    try:\n",
    "        response = client.embeddings(model=model, input=[text])\n",
    "        embedding = response.data[0].embedding\n",
    "        usage = response.usage\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"embedding\": embedding,\n",
    "            \"usage\": usage\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "def get_embeddings_batch(texts, model=\"mistral-embed\"):\n",
    "    \"\"\"Get embeddings for multiple texts in a batch.\"\"\"\n",
    "    try:\n",
    "        response = client.embeddings(model=model, input=texts)\n",
    "        embeddings = [data.embedding for data in response.data]\n",
    "        usage = response.usage\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"embeddings\": embeddings,\n",
    "            \"usage\": usage\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def find_most_similar(query_embedding, embeddings, texts, top_k=3):\n",
    "    \"\"\"Find the most similar texts to a query embedding.\"\"\"\n",
    "    similarities = [cosine_similarity(query_embedding, emb) for emb in embeddings]\n",
    "    most_similar_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    \n",
    "    return [\n",
    "        {\"text\": texts[i], \"similarity\": similarities[i], \"index\": i}\n",
    "        for i in most_similar_indices\n",
    "    ]\n",
    "\n",
    "# Test embedding model with sample texts\n",
    "print(\"üîç Testing embedding capabilities\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "sample_texts = [\n",
    "    \"Artificial intelligence is transforming industries worldwide.\",\n",
    "    \"Machine learning allows computers to improve automatically through experience.\",\n",
    "    \"Neural networks are computing systems inspired by the human brain.\",\n",
    "    \"Climate change is causing global temperatures to rise.\",\n",
    "    \"Renewable energy sources include solar, wind, and hydroelectric power.\",\n",
    "    \"Sustainable development meets present needs without compromising future generations.\",\n",
    "    \"Quantum computing uses quantum mechanics to process information.\",\n",
    "    \"Blockchain is a distributed ledger technology with many applications.\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Get embeddings for all sample texts\n",
    "    result = get_embeddings_batch(sample_texts)\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        embeddings = result[\"embeddings\"]\n",
    "        usage = result[\"usage\"]\n",
    "        \n",
    "        print(f\"‚úÖ Generated embeddings for {len(embeddings)} texts\")\n",
    "        print(f\"üìä Usage: {usage.prompt_tokens} tokens\")\n",
    "        print(f\"üìä Embedding dimensions: {len(embeddings[0])}\")\n",
    "        \n",
    "        # Calculate cost\n",
    "        input_cost, output_cost, total_cost = estimate_cost(\"mistral-embed\", usage, df_pricing)\n",
    "        print(f\"üí∞ Estimated cost: ${total_cost:.6f}\")\n",
    "        \n",
    "        # Example query for semantic search\n",
    "        query_texts = [\n",
    "            \"AI and machine learning technologies\",\n",
    "            \"Environmental sustainability and clean energy\",\n",
    "            \"Advanced computing technologies\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nüîé Semantic Search Examples:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for query_text in query_texts:\n",
    "            print(f\"\\nQuery: '{query_text}'\")\n",
    "            \n",
    "            # Get embedding for query\n",
    "            query_result = get_embedding(query_text)\n",
    "            \n",
    "            if query_result[\"success\"]:\n",
    "                query_embedding = query_result[\"embedding\"]\n",
    "                \n",
    "                # Find most similar texts\n",
    "                similar_texts = find_most_similar(query_embedding, embeddings, sample_texts, top_k=3)\n",
    "                \n",
    "                print(\"Most relevant texts:\")\n",
    "                for i, item in enumerate(similar_texts, 1):\n",
    "                    print(f\"  {i}. {item['text']} (similarity: {item['similarity']:.4f})\")\n",
    "            else:\n",
    "                print(f\"‚ùå Query embedding failed: {query_result['error']}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Embedding generation failed: {result['error']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Embedding test error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb3e07e",
   "metadata": {},
   "source": [
    "## 5) Quick price comparison plots\n",
    "\n",
    "These charts visualize **input** and **output** token prices for representative models in the Mistral AI family (per 1M tokens).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e72229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a subset of models for comparison\n",
    "text_models_df = df_pricing[df_pricing['category'] == \"text\"]\n",
    "main_models = text_models_df[text_models_df['model'].str.contains('latest|2407') & ~text_models_df['model'].str.contains('open-')]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create a bar chart for input prices\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(main_models['model'], main_models['input'], color='skyblue', alpha=0.7)\n",
    "plt.title(\"Input price per 1M tokens\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"USD\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Create a bar chart for output prices\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(main_models['model'], main_models['output'], color='lightcoral', alpha=0.7)\n",
    "plt.title(\"Output price per 1M tokens\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"USD\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Now let's create a cost efficiency comparison (input token price per context size)\n",
    "models = main_models['model']\n",
    "inputs = main_models['input']\n",
    "outputs = main_models['output']\n",
    "contexts = main_models['context_length']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Total cost for 1:3 ratio (input:output) of 1M tokens\n",
    "total_costs = inputs + (outputs * 3)\n",
    "\n",
    "# Plot the total costs\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(models, total_costs, color='lightgreen', alpha=0.7)\n",
    "plt.title(\"Total cost per 1M input tokens + 3M output tokens\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"USD\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot the cost efficiency (value = context length / total cost)\n",
    "cost_efficiency = contexts / total_costs\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(models, cost_efficiency, color='lightblue', alpha=0.7)\n",
    "plt.title(\"Cost efficiency (context length / total cost)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"Context tokens per USD\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035034ca",
   "metadata": {},
   "source": [
    "## 6) Streaming Tests\n",
    "\n",
    "Test the streaming capabilities of Mistral AI models to receive responses token by token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "def run_streaming_chat(model, messages, temperature=0.7, max_tokens=None, display_interval=0.05):\n",
    "    \"\"\"Run a streaming chat and display tokens as they arrive.\"\"\"\n",
    "    assert client is not None, \"MistralClient not initialized. Set MISTRAL_API_KEY and initialize client.\"\n",
    "    \n",
    "    try:\n",
    "        # Start the streaming response\n",
    "        print(f\"üîÑ Streaming response from {model}...\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        response = \"\"\n",
    "        start_time = time.time()\n",
    "        token_count = 0\n",
    "        \n",
    "        # Call the streaming endpoint\n",
    "        for chunk in client.chat_stream(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        ):\n",
    "            # Get the new token\n",
    "            if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:\n",
    "                new_content = chunk.choices[0].delta.content\n",
    "                response += new_content\n",
    "                token_count += 1\n",
    "                \n",
    "                # Display the response so far\n",
    "                clear_output(wait=True)\n",
    "                print(f\"üîÑ Streaming response from {model}... ({token_count} tokens, {time.time() - start_time:.2f}s)\")\n",
    "                print(\"-\" * 50)\n",
    "                print(response)\n",
    "                \n",
    "                # Add a small delay to make the streaming visible\n",
    "                time.sleep(display_interval)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        tokens_per_second = token_count / elapsed_time if elapsed_time > 0 else 0\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "        print(f\"‚úÖ Streaming completed in {elapsed_time:.2f} seconds\")\n",
    "        print(f\"üìä Tokens generated: {token_count}\")\n",
    "        print(f\"‚ö° Tokens per second: {tokens_per_second:.2f}\")\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"content\": response,\n",
    "            \"tokens\": token_count,\n",
    "            \"elapsed_time\": elapsed_time\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Streaming error: {e}\")\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "# Test streaming with a creative writing prompt\n",
    "streaming_prompt = \"Write a short story about an AI assistant that learns to paint. Make it creative and engaging.\"\n",
    "messages = [ChatMessage(role=\"user\", content=streaming_prompt)]\n",
    "\n",
    "# Uncomment to run the streaming test\n",
    "# streaming_result = run_streaming_chat(\"mistral-medium-latest\", messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1e8390",
   "metadata": {},
   "source": [
    "## 7) Tool Use and Function Calling Tests\n",
    "\n",
    "Test Mistral AI's ability to use tools and call functions when provided with tool definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b94cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from mistralai.models.chat_completion import ChatFunction, ChatFunctionParameters\n",
    "\n",
    "# Define some sample functions\n",
    "weather_function = ChatFunction(\n",
    "    name=\"get_weather\",\n",
    "    description=\"Get the current weather in a given location\",\n",
    "    parameters=ChatFunctionParameters(\n",
    "        properties={\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "            },\n",
    "            \"unit\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                \"description\": \"The temperature unit to use\",\n",
    "            },\n",
    "        },\n",
    "        required=[\"location\"],\n",
    "    ),\n",
    ")\n",
    "\n",
    "calculator_function = ChatFunction(\n",
    "    name=\"calculate\",\n",
    "    description=\"Calculate the result of a mathematical expression\",\n",
    "    parameters=ChatFunctionParameters(\n",
    "        properties={\n",
    "            \"expression\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The mathematical expression to evaluate, e.g. '2 + 2'\",\n",
    "            },\n",
    "        },\n",
    "        required=[\"expression\"],\n",
    "    ),\n",
    ")\n",
    "\n",
    "database_function = ChatFunction(\n",
    "    name=\"query_database\",\n",
    "    description=\"Query a database for information\",\n",
    "    parameters=ChatFunctionParameters(\n",
    "        properties={\n",
    "            \"table\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The table to query\",\n",
    "            },\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The query to run\",\n",
    "            },\n",
    "            \"limit\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"description\": \"The maximum number of results to return\",\n",
    "            },\n",
    "        },\n",
    "        required=[\"table\", \"query\"],\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Function to execute tool calling\n",
    "def run_with_tools(model, prompt, functions, temperature=0.7):\n",
    "    \"\"\"Run a chat with tool definitions and handle the function calls.\"\"\"\n",
    "    try:\n",
    "        messages = [ChatMessage(role=\"user\", content=prompt)]\n",
    "        \n",
    "        print(f\"üîß Running {model} with tools...\")\n",
    "        print(f\"üîç Prompt: {prompt}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Call the API with functions\n",
    "        response = client.chat(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            tools=functions\n",
    "        )\n",
    "        \n",
    "        message = response.choices[0].message\n",
    "        content = message.content\n",
    "        tool_calls = message.tool_calls\n",
    "        \n",
    "        print(f\"üí¨ Response content: {content}\")\n",
    "        \n",
    "        if tool_calls:\n",
    "            print(f\"\\nüß∞ Tool calls detected: {len(tool_calls)}\")\n",
    "            \n",
    "            for idx, tool_call in enumerate(tool_calls):\n",
    "                print(f\"\\nTool call {idx+1}:\")\n",
    "                print(f\"  Function: {tool_call.function.name}\")\n",
    "                print(f\"  Arguments: {tool_call.function.arguments}\")\n",
    "                \n",
    "                # Parse arguments\n",
    "                try:\n",
    "                    args = json.loads(tool_call.function.arguments)\n",
    "                    print(f\"  Parsed arguments: {args}\")\n",
    "                except:\n",
    "                    print(f\"  Warning: Could not parse arguments as JSON\")\n",
    "        else:\n",
    "            print(\"\\nNo tool calls made in this response.\")\n",
    "        \n",
    "        # Return the result\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"content\": content,\n",
    "            \"tool_calls\": tool_calls,\n",
    "            \"usage\": response.usage\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Tool calling error: {e}\")\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "# Test with different prompts that should trigger tool usage\n",
    "tool_prompts = [\n",
    "    \"What's the weather like in Paris?\",\n",
    "    \"Calculate 125 * 37 - 42\",\n",
    "    \"Can you query the customers table for all users who signed up last month? Limit to 10 results.\"\n",
    "]\n",
    "\n",
    "for prompt in tool_prompts:\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        # We'll use mistral-large-latest for best tool use capabilities\n",
    "        result = run_with_tools(\n",
    "            \"mistral-large-latest\", \n",
    "            prompt, \n",
    "            [weather_function, calculator_function, database_function], \n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            # Calculate cost\n",
    "            input_cost, output_cost, total_cost = estimate_cost(\"mistral-large-latest\", result[\"usage\"], df_pricing)\n",
    "            print(f\"\\nüìä Usage:\")\n",
    "            print(f\"  Prompt tokens: {result['usage'].prompt_tokens}\")\n",
    "            print(f\"  Completion tokens: {result['usage'].completion_tokens}\")\n",
    "            print(f\"  Estimated cost: ${total_cost:.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error testing prompt '{prompt}': {e}\")\n",
    "    \n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c15a87",
   "metadata": {},
   "source": [
    "## 8) System Prompting and Parameter Fine-tuning\n",
    "\n",
    "Test the impact of different system prompts and parameter settings on model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95f3b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the impact of system prompts\n",
    "def test_system_prompt(user_prompt, system_prompts, model=\"mistral-medium-latest\", temperature=0.7):\n",
    "    \"\"\"Test the same user prompt with different system prompts.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(f\"üß™ Testing system prompts with user prompt: '{user_prompt}'\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, system_prompt in enumerate(system_prompts, 1):\n",
    "        try:\n",
    "            print(f\"\\nüìù System prompt {i}: '{system_prompt}'\")\n",
    "            \n",
    "            messages = [\n",
    "                ChatMessage(role=\"system\", content=system_prompt),\n",
    "                ChatMessage(role=\"user\", content=user_prompt)\n",
    "            ]\n",
    "            \n",
    "            result = run_chat_with_model(model, messages, temperature)\n",
    "            \n",
    "            if result[\"success\"]:\n",
    "                print(f\"\\nüí¨ Response:\")\n",
    "                print(result[\"content\"])\n",
    "                \n",
    "                results[system_prompt] = result\n",
    "                print(f\"\\nüìä Tokens: {result['usage'].completion_tokens}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Error: {result['error']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with system prompt: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the impact of different parameter settings\n",
    "def test_parameters(prompt, parameters, model=\"mistral-medium-latest\"):\n",
    "    \"\"\"Test the same prompt with different parameter settings.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(f\"üß™ Testing parameters with prompt: '{prompt}'\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, param_set in enumerate(parameters, 1):\n",
    "        try:\n",
    "            param_desc = \", \".join([f\"{k}={v}\" for k, v in param_set.items()])\n",
    "            print(f\"\\n‚öôÔ∏è Parameter set {i}: {param_desc}\")\n",
    "            \n",
    "            messages = [ChatMessage(role=\"user\", content=prompt)]\n",
    "            \n",
    "            response = client.chat(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                **param_set\n",
    "            )\n",
    "            \n",
    "            content = response.choices[0].message.content\n",
    "            usage = response.usage\n",
    "            \n",
    "            print(f\"\\nüí¨ Response:\")\n",
    "            print(content[:500] + (\"...\" if len(content) > 500 else \"\"))\n",
    "            \n",
    "            results[param_desc] = {\n",
    "                \"content\": content,\n",
    "                \"usage\": usage,\n",
    "                \"finish_reason\": response.choices[0].finish_reason\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nüìä Completion tokens: {usage.completion_tokens}\")\n",
    "            print(f\"üìä Finish reason: {response.choices[0].finish_reason}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with parameter set: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with different system prompts\n",
    "user_prompt = \"Explain how generative AI works in simple terms.\"\n",
    "\n",
    "system_prompts = [\n",
    "    \"You are a helpful assistant that explains complex topics in simple terms.\",\n",
    "    \"You are an expert AI researcher explaining concepts to other researchers.\",\n",
    "    \"You are a teacher explaining technology to 10-year-old students.\",\n",
    "    \"You are a comedian who uses humor to explain complex topics.\"\n",
    "]\n",
    "\n",
    "print(\"üî¨ System Prompt Test\")\n",
    "print(\"=\" * 80)\n",
    "system_results = test_system_prompt(user_prompt, system_prompts)\n",
    "\n",
    "# Test with different parameters\n",
    "creative_prompt = \"Write a short poem about artificial intelligence.\"\n",
    "\n",
    "parameter_sets = [\n",
    "    {\"temperature\": 0.1, \"top_p\": 1.0},\n",
    "    {\"temperature\": 0.7, \"top_p\": 1.0},\n",
    "    {\"temperature\": 1.2, \"top_p\": 1.0},\n",
    "    {\"temperature\": 0.7, \"top_p\": 0.5},\n",
    "    {\"temperature\": 0.7, \"top_p\": 1.0, \"max_tokens\": 50}\n",
    "]\n",
    "\n",
    "print(\"\\nüî¨ Parameter Test\")\n",
    "print(\"=\" * 80)\n",
    "parameter_results = test_parameters(creative_prompt, parameter_sets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb17a2",
   "metadata": {},
   "source": [
    "## 9) Fine-tuning and Custom Models\n",
    "\n",
    "Information on Mistral's capabilities for fine-tuning and customizing models for specific use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d78df1d",
   "metadata": {},
   "source": [
    "### Fine-tuning options with Mistral AI\n",
    "\n",
    "While this notebook focuses on testing the available API models, Mistral AI also offers fine-tuning capabilities for enterprises. Fine-tuning allows you to customize models for specific use cases, domain-specific language, and custom behaviors.\n",
    "\n",
    "#### Key fine-tuning advantages:\n",
    "\n",
    "1. **Domain-specific knowledge**: Train models to excel in your industry vocabulary and context\n",
    "2. **Company-specific tone and style**: Customize output format and communication style\n",
    "3. **Improved accuracy**: Better results for your particular use cases\n",
    "4. **Custom behaviors**: Define how the model handles specific types of queries\n",
    "5. **Improved efficiency**: Fine-tuned models often require fewer tokens for context\n",
    "\n",
    "#### Enterprise deployment options:\n",
    "\n",
    "- **Private cloud deployments**\n",
    "- **On-premises solutions**\n",
    "- **Custom API endpoints for your fine-tuned models**\n",
    "\n",
    "For more information on fine-tuning options, contact Mistral AI's enterprise sales team through their website: https://mistral.ai/\n",
    "\n",
    "#### Open models for self-hosting\n",
    "\n",
    "Mistral AI also offers open models that can be self-hosted or fine-tuned independently:\n",
    "\n",
    "- **Open-mistral-7b**: 7 billion parameter model for general use\n",
    "- **Open-mixtral-8x7b**: Mixture of experts model with 8 experts each with 7B parameters\n",
    "\n",
    "These models are available on the Mistral AI API but can also be downloaded and deployed on your own infrastructure from Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c64c52",
   "metadata": {},
   "source": [
    "## 10) Advanced Response Formatting\n",
    "\n",
    "Test Mistral AI's ability to follow formatting instructions for structured outputs like JSON and markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8aefd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_structured_output(prompt, format_instruction, model=\"mistral-medium-latest\", temperature=0.3):\n",
    "    \"\"\"Test the model's ability to produce structured output formats.\"\"\"\n",
    "    try:\n",
    "        system_prompt = f\"You must respond in the format specified: {format_instruction}\"\n",
    "        \n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=system_prompt),\n",
    "            ChatMessage(role=\"user\", content=prompt)\n",
    "        ]\n",
    "        \n",
    "        print(f\"üß™ Testing structured output with {model}\")\n",
    "        print(f\"üìù Format: {format_instruction}\")\n",
    "        print(f\"üìù Prompt: {prompt}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        result = run_chat_with_model(model, messages, temperature)\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            print(f\"\\nüí¨ Response:\")\n",
    "            print(result[\"content\"])\n",
    "            \n",
    "            # Try to validate the format\n",
    "            if \"JSON\" in format_instruction.upper():\n",
    "                try:\n",
    "                    # Attempt to parse as JSON\n",
    "                    parsed = json.loads(result[\"content\"])\n",
    "                    print(f\"\\n‚úÖ Valid JSON format detected. Parsed structure:\")\n",
    "                    print(json.dumps(parsed, indent=2))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"\\n‚ùå Invalid JSON format: {e}\")\n",
    "            \n",
    "            # Calculate cost\n",
    "            input_cost, output_cost, total_cost = estimate_cost(model, result[\"usage\"], df_pricing)\n",
    "            print(f\"\\nüìä Usage:\")\n",
    "            print(f\"  Prompt tokens: {result['usage'].prompt_tokens}\")\n",
    "            print(f\"  Completion tokens: {result['usage'].completion_tokens}\")\n",
    "            print(f\"  Estimated cost: ${total_cost:.6f}\")\n",
    "            \n",
    "            return {\"success\": True, \"content\": result[\"content\"], \"usage\": result[\"usage\"]}\n",
    "        else:\n",
    "            print(f\"‚ùå Error: {result['error']}\")\n",
    "            return {\"success\": False, \"error\": result[\"error\"]}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "# Test different structured output formats\n",
    "format_tests = [\n",
    "    {\n",
    "        \"prompt\": \"List the top 3 programming languages with their pros and cons\",\n",
    "        \"format\": \"Respond with a valid JSON array containing objects with 'language', 'pros', and 'cons' fields. The 'pros' and 'cons' should each be arrays of strings.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Compare electric cars to gas-powered cars\",\n",
    "        \"format\": \"Respond with a markdown table with columns for 'Feature', 'Electric Cars', and 'Gas-Powered Cars'.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Create a customer profile for an e-commerce website\",\n",
    "        \"format\": \"Respond with a JSON object containing 'customer_id', 'name', 'age', 'email', 'purchase_history' (array), and 'preferences' (object).\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test with a capable model\n",
    "model_for_structured = \"mistral-large-latest\"\n",
    "\n",
    "for test in format_tests:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    test_structured_output(test[\"prompt\"], test[\"format\"], model_for_structured)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c0a0df",
   "metadata": {},
   "source": [
    "## 11) Context Length Tests\n",
    "\n",
    "Test Mistral AI models with increasing context lengths to observe behavior with longer inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e39a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_of_length(target_token_count):\n",
    "    \"\"\"Generate a text of approximately the target token count.\"\"\"\n",
    "    # A rough approximation of tokens to words (1 token ‚âà 0.75 words)\n",
    "    word_count = int(target_token_count * 0.75)\n",
    "    \n",
    "    # Generate a repeating text pattern\n",
    "    base_text = \"\"\"This is a sample text used to test the context length capabilities of language models. \n",
    "    It contains various words and sentences to simulate a realistic document. \n",
    "    The content itself is not particularly meaningful, but it serves the purpose of testing how models handle \n",
    "    longer inputs and if they maintain coherence throughout the processing of extensive texts. \n",
    "    We'll repeat this paragraph multiple times to reach the desired token count.\"\"\"\n",
    "    \n",
    "    # Calculate how many repetitions we need\n",
    "    words_per_repeat = len(base_text.split())\n",
    "    repeats_needed = max(1, word_count // words_per_repeat + 1)\n",
    "    \n",
    "    # Generate the text with paragraph numbers\n",
    "    text = \"\"\n",
    "    for i in range(repeats_needed):\n",
    "        text += f\"\\n\\nParagraph {i+1}:\\n{base_text}\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "def test_context_length(model, token_lengths, query=\"Summarize the above text in 3-4 sentences.\"):\n",
    "    \"\"\"Test the model with different context lengths.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(f\"üìè Testing {model} with different context lengths\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for length in token_lengths:\n",
    "        try:\n",
    "            print(f\"\\nüîç Testing with approximately {length} tokens\")\n",
    "            \n",
    "            # Generate text of the target length\n",
    "            context = generate_text_of_length(length)\n",
    "            prompt = context + \"\\n\\n\" + query\n",
    "            \n",
    "            # Run the test\n",
    "            start_time = time.time()\n",
    "            result = run_text_prompt(model, prompt, temperature=0.3)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            if result[\"success\"]:\n",
    "                actual_tokens = result[\"usage\"].prompt_tokens\n",
    "                print(f\"‚úÖ Successful response with {actual_tokens} actual prompt tokens\")\n",
    "                print(f\"‚è±Ô∏è Processing time: {elapsed_time:.2f} seconds\")\n",
    "                print(f\"üìù Response: {result['content'][:200]}...\")\n",
    "                \n",
    "                # Calculate processing speed\n",
    "                tokens_per_second = actual_tokens / elapsed_time if elapsed_time > 0 else 0\n",
    "                \n",
    "                # Calculate cost\n",
    "                input_cost, output_cost, total_cost = estimate_cost(model, result[\"usage\"], df_pricing)\n",
    "                \n",
    "                results[length] = {\n",
    "                    \"actual_tokens\": actual_tokens,\n",
    "                    \"elapsed_time\": elapsed_time,\n",
    "                    \"tokens_per_second\": tokens_per_second,\n",
    "                    \"total_cost\": total_cost\n",
    "                }\n",
    "                \n",
    "                print(f\"‚ö° Processing speed: {tokens_per_second:.2f} tokens/second\")\n",
    "                print(f\"üí∞ Estimated cost: ${total_cost:.6f}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Error: {result['error']}\")\n",
    "                results[length] = {\"error\": result[\"error\"]}\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with {length} tokens: {e}\")\n",
    "            results[length] = {\"error\": str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with a range of context lengths\n",
    "token_lengths = [500, 1000, 2000, 5000]\n",
    "\n",
    "# We'll test with mistral-small-latest which has a 32k context window\n",
    "context_test_model = \"mistral-small-latest\"\n",
    "\n",
    "# Uncomment to run the context length test\n",
    "# context_results = test_context_length(context_test_model, token_lengths)\n",
    "\n",
    "# Visualize the results (will only work if you've run the test)\n",
    "def visualize_context_results(results):\n",
    "    \"\"\"Create visualizations for context length test results.\"\"\"\n",
    "    if not results or not any('actual_tokens' in results[k] for k in results):\n",
    "        print(\"No valid test results available for visualization.\")\n",
    "        return\n",
    "    \n",
    "    # Extract data for plotting\n",
    "    token_counts = []\n",
    "    processing_times = []\n",
    "    tokens_per_second = []\n",
    "    costs = []\n",
    "    \n",
    "    for length, data in results.items():\n",
    "        if 'actual_tokens' in data:\n",
    "            token_counts.append(data['actual_tokens'])\n",
    "            processing_times.append(data['elapsed_time'])\n",
    "            tokens_per_second.append(data['tokens_per_second'])\n",
    "            costs.append(data['total_cost'])\n",
    "    \n",
    "    # Create plots\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Processing time vs. Token count\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(token_counts, processing_times, 'o-', color='blue')\n",
    "    plt.title('Processing Time vs. Token Count')\n",
    "    plt.xlabel('Actual Token Count')\n",
    "    plt.ylabel('Processing Time (seconds)')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Processing speed vs. Token count\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(token_counts, tokens_per_second, 'o-', color='green')\n",
    "    plt.title('Processing Speed vs. Token Count')\n",
    "    plt.xlabel('Actual Token Count')\n",
    "    plt.ylabel('Tokens per Second')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Cost vs. Token count\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(token_counts, costs, 'o-', color='red')\n",
    "    plt.title('Cost vs. Token Count')\n",
    "    plt.xlabel('Actual Token Count')\n",
    "    plt.ylabel('Cost (USD)')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Cost efficiency (tokens per dollar)\n",
    "    cost_efficiency = [count/cost if cost > 0 else 0 for count, cost in zip(token_counts, costs)]\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(token_counts, cost_efficiency, 'o-', color='purple')\n",
    "    plt.title('Cost Efficiency vs. Token Count')\n",
    "    plt.xlabel('Actual Token Count')\n",
    "    plt.ylabel('Tokens per Dollar')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment to visualize the results if you've run the test\n",
    "# if 'context_results' in locals():\n",
    "#     visualize_context_results(context_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1adf31",
   "metadata": {},
   "source": [
    "## 12) Performance Comparison with OpenAI Models\n",
    "\n",
    "A high-level comparison of Mistral AI models with corresponding OpenAI models for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc4ee7",
   "metadata": {},
   "source": [
    "### Comparing Mistral and OpenAI Models\n",
    "\n",
    "This table provides a rough comparison between Mistral AI and OpenAI models based on capabilities and pricing. Note that exact performance can vary by task.\n",
    "\n",
    "| Mistral Model | Similar OpenAI Model | Mistral Price (1M tokens) | OpenAI Price (1M tokens) | Notes |\n",
    "|---------------|----------------------|---------------------------|--------------------------|-------|\n",
    "| mistral-large-latest | gpt-4o | $8 in / $24 out | $2.50 in / $10 out | Mistral Large provides advanced reasoning and comprehensive capabilities, though OpenAI's GPT-4o offers multimodal features at a lower price point |\n",
    "| mistral-medium-latest | gpt-4o-mini | $2.70 in / $8.10 out | $0.15 in / $0.60 out | Mistral Medium balances capability and cost, but GPT-4o-mini is significantly more cost-effective |\n",
    "| mistral-small-latest | gpt-4.1-mini | $0.70 in / $2.10 out | $0.40 in / $1.60 out | Closer in pricing, with Mistral Small slightly more expensive on input but more competitive on output |\n",
    "| mistral-tiny-2407 | gpt-5-nano | $0.14 in / $0.42 out | $0.05 in / $0.40 out | Both are efficient models for simple tasks; OpenAI's model has cheaper input costs |\n",
    "| mistral-large-vision-2407 | gpt-4o | $8 in / $24 out | $2.50 in / $10 out | Both handle vision tasks; OpenAI's model is more cost-effective |\n",
    "| mistral-embed | text-embedding-3-small | $0.10 in | $0.01 in | OpenAI's embedding model is more cost-effective |\n",
    "\n",
    "### Key Considerations When Choosing Between Providers:\n",
    "\n",
    "1. **Cost structure**: Mistral generally has higher per-token pricing, especially for smaller models\n",
    "2. **Context length**: Mistral offers 32K context windows across most models\n",
    "3. **Data privacy**: Mistral emphasizes privacy with clear data usage policies\n",
    "4. **Model performance**: Performance varies by task; benchmarking for your specific use case is recommended\n",
    "5. **Diversification**: Using multiple providers can reduce dependency risks\n",
    "6. **European alternative**: Mistral provides a European-based LLM option\n",
    "7. **Open models**: Mistral offers open-weight models that can be self-hosted\n",
    "\n",
    "### When to Choose Mistral AI:\n",
    "\n",
    "- When data privacy and sovereignty are critical concerns\n",
    "- When you need a European AI provider for compliance reasons\n",
    "- For tasks where Mistral models excel in quality (domain-specific use cases)\n",
    "- When you want access to both API and open-weight versions of similar models\n",
    "- For applications requiring long context windows without paying premium pricing tiers\n",
    "\n",
    "### When to Choose OpenAI:\n",
    "\n",
    "- When cost efficiency is a primary concern, especially at scale\n",
    "- For multimodal applications requiring advanced vision capabilities\n",
    "- When using the most cutting-edge models is a priority\n",
    "- For applications that benefit from OpenAI's more extensive tooling ecosystem\n",
    "- When fine-tuned custom GPT models are preferred\n",
    "\n",
    "For production applications, we recommend benchmarking both providers on your specific tasks to determine the best fit for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24536bdc",
   "metadata": {},
   "source": [
    "## Conclusion and Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Model Selection**: Choose the appropriate model based on your task complexity and budget:\n",
    "   - **mistral-large-latest**: For complex reasoning, nuanced understanding, and highest quality outputs\n",
    "   - **mistral-medium-latest**: Good balance of capability and cost for most general applications\n",
    "   - **mistral-small-latest**: Cost-effective for straightforward tasks and content generation\n",
    "   - **mistral-tiny-2407**: Most efficient for simple tasks where basic comprehension is sufficient\n",
    "\n",
    "2. **Cost Optimization**:\n",
    "   - Mistral models are priced differently for input vs. output tokens, with output being more expensive\n",
    "   - Optimize prompts to be concise while providing necessary context\n",
    "   - Consider batching operations when possible\n",
    "   - For high-volume applications, compare with other providers to find the most cost-effective solution\n",
    "\n",
    "3. **Parameter Tuning**:\n",
    "   - Lower temperature (0.1-0.3) for factual, consistent responses\n",
    "   - Higher temperature (0.7-1.0) for creative content\n",
    "   - Use system prompts to guide model behavior effectively\n",
    "   - Experiment with top_p settings for the right balance of creativity and coherence\n",
    "\n",
    "4. **Context Length Efficiency**:\n",
    "   - All Mistral models support long contexts (up to 32K tokens)\n",
    "   - Processing time increases with context length\n",
    "   - Structure long-context prompts carefully to ensure the model focuses on the most relevant information\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Effective Prompting**:\n",
    "   - Be specific and clear in your instructions\n",
    "   - Use system prompts for consistent model behavior\n",
    "   - Structure complex prompts with clear sections\n",
    "   - Provide examples for desired output formats\n",
    "\n",
    "2. **Production Implementation**:\n",
    "   - Implement robust error handling for API calls\n",
    "   - Use streaming for better user experience with longer generations\n",
    "   - Cache frequently requested content to reduce API calls\n",
    "   - Monitor usage to avoid unexpected costs\n",
    "\n",
    "3. **Tool Use and Function Calling**:\n",
    "   - Define clear function schemas with precise parameter descriptions\n",
    "   - Use low temperature (0.2-0.3) for more reliable tool use\n",
    "   - Validate function arguments before execution\n",
    "   - Structure complex workflows as sequences of tool calls\n",
    "\n",
    "4. **Multimodal Applications**:\n",
    "   - Provide clear instructions for image analysis\n",
    "   - Consider image resolution and format for optimal processing\n",
    "   - Be specific about what aspects of the image to analyze\n",
    "\n",
    "5. **Embeddings and Semantic Search**:\n",
    "   - Normalize embeddings for consistent similarity comparisons\n",
    "   - Consider dimensionality vs. accuracy tradeoffs\n",
    "   - Use batch processing for efficiency\n",
    "   - Implement vector databases for production-scale applications\n",
    "\n",
    "By following these guidelines and utilizing the tests in this notebook, you can effectively leverage Mistral AI's models for a wide range of applications while optimizing for quality, cost, and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0055174d",
   "metadata": {},
   "source": [
    "## Appendix: Raw pricing DataFrame\n",
    "\n",
    "Re-display the pricing table for quick reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0faa83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pricing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}",
    "